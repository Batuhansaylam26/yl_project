_wandb:
    value:
        cli_version: 0.24.0
        e:
            q0jk5ravm67k4em5ohqf3nlhk69oa80f:
                args:
                    - --val_size
                    - "1000"
                codePath: src/main.py
                codePathLocal: src/main.py
                cpu_count: 10
                cpu_count_logical: 10
                disk:
                    /:
                        total: "240120725504"
                        used: "33958801408"
                email: batuhansaylam1071@gmail.com
                executable: /usr/local/bin/python
                git:
                    commit: 1cf07d1c4d5fe671030be4aaa9796e30dd7e772f
                    remote: https://github.com/Batuhansaylam26/yl_project.git
                host: d27327059c6b
                memory:
                    total: "16484704256"
                os: Linux-6.12.54-linuxkit-aarch64-with-glibc2.41
                program: /workspaces/yl_project/src/main.py
                python: CPython 3.12.12
                root: .
                startedAt: "2026-01-16T00:25:49.892995Z"
                writerId: q0jk5ravm67k4em5ohqf3nlhk69oa80f
        m:
            - "1": trainer/global_step
              "6":
                - 3
              "7": []
            - "2": '*'
              "5": 1
              "6":
                - 1
              "7": []
        python_version: 3.12.12
        t:
            "1":
                - 1
                - 5
                - 9
                - 30
                - 35
                - 53
                - 103
                - 106
            "2":
                - 1
                - 5
                - 9
                - 30
                - 35
                - 53
                - 103
                - 106
            "3":
                - 7
                - 66
            "4": 3.12.12
            "5": 0.24.0
            "12": 0.24.0
            "13": linux-aarch64
accelerator:
    value: auto
alias:
    value: null
batch_size:
    value: 64
callbacks:
    value:
        - <pytorch_lightning.callbacks.early_stopping.EarlyStopping object at 0xfffed813e480>
        - <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0xfffed813e420>
conv_hidden_size:
    value: 128
dataloader_kwargs:
    value: null
drop_last_loader:
    value: false
dropout:
    value: 0.2783507796393389
early_stop_patience_steps:
    value: 5
enable_checkpointing:
    value: true
enable_model_summary:
    value: true
enable_progress_bar:
    value: true
encoder_layers:
    value: 3
exclude_insample_y:
    value: false
futr_exog_list:
    value: null
h:
    value: 20
h_train:
    value: 1
hidden_size:
    value: 128
hist_exog_list:
    value: null
inference_input_size:
    value: 72
inference_windows_batch_size:
    value: 256
input_size:
    value: 72
learning_rate:
    value: 0.0001
logger:
    value: <lightning.pytorch.loggers.wandb.WandbLogger object at 0xfffed813dd60>
loss:
    value: null
lr_scheduler:
    value: CosineAnnealingLR
lr_scheduler_kwargs:
    value:
        T_max: 10
max_steps:
    value: 1000
n_samples:
    value: 100
n_series:
    value: 1
num_kernels:
    value: 5
num_lr_decays:
    value: -1
optimizer:
    value: Adadelta
optimizer_kwargs:
    value:
        lr: 0.001
        rho: 0.75
random_seed:
    value: 26
scaler_type:
    value: standard
start_padding_enabled:
    value: false
stat_exog_list:
    value: null
step_size:
    value: 1
top_k:
    value: 5
training_data_availability_threshold:
    value: 0
val_check_steps:
    value: 10
valid_batch_size:
    value: null
valid_loss:
    value: null
windows_batch_size:
    value: 64
