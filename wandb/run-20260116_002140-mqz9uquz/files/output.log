[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:625: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate
  warnings.warn(

  | Name           | Type          | Params | Mode  | FLOPs
-----------------------------------------------------------------
0 | loss           | MSE           | 0      | train | 0
1 | valid_loss     | MSE           | 0      | train | 0
2 | padder_train   | ConstantPad1d | 0      | train | 0
3 | scaler         | TemporalNorm  | 0      | train | 0
4 | model          | ModuleList    | 1.2 M  | train | 0
5 | enc_embedding  | DataEmbedding | 384    | train | 0
6 | layer_norm     | LayerNorm     | 256    | train | 0
7 | predict_linear | Linear        | 1.1 K  | train | 0
8 | projection     | Linear        | 129    | train | 0
-----------------------------------------------------------------
1.2 M     Trainable params
0         Non-trainable params
1.2 M     Total params
4.697     Total estimated model params size (MB)
32        Modules in train mode
0         Modules in eval mode
0         Total Flops
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
Epoch 44:   0%|               | 0/1 [00:00<?, ?it/s, v_num=uquz, train_loss_step=12.80, train_loss_epoch=12.80, valid_loss=1.070]
                                                                                                                                 
Metric valid_loss improved. New best score: 1.066
Epoch 9, global step 10: 'valid_loss' reached 1.06635 (best 1.06635), saving model to '/workspaces/yl_project/checkpoints/best-TimesNet-epoch=09-valid_loss=1.0663.ckpt' as top 1
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
[36m(pid=gcs_server)[0m [2026-01-16 00:22:05,979 E 62052 62052] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[33m(raylet)[0m [2026-01-16 00:22:08,028 E 62308 62308] (raylet) main.cc:1032: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
Epoch 19, global step 20: 'valid_loss' was not in top 1
[36m(pid=62391)[0m [2026-01-16 00:22:09,198 E 62391 62479] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
Epoch 29, global step 30: 'valid_loss' was not in top 1
Epoch 39, global step 40: 'valid_loss' was not in top 1

Detected KeyboardInterrupt, attempting graceful shutdown ...
