[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:625: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate
  warnings.warn(
[33m[W 2026-01-15 23:52:15,800][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 256, 'dropout': 0.3421812787676858, 'conv_hidden_size': 64, 'top_k': 6, 'num_kernels': 7, 'encoder_layers': 2, 'batch_size': 16, 'windows_batch_size': 512} because of the following error: TypeError("'<=' not supported between instances of 'float' and 'Float'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 602, in _fit
    trainer.fit(model, datamodule=datamodule)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1053, in _run
    self.strategy.setup(self)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 159, in setup
    self.setup_optimizers(trainer)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 180, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 629, in configure_optimizers
    optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/torch/optim/adadelta.py", line 44, in __init__
    if not 0.0 <= lr:
           ^^^^^^^^^
TypeError: '<=' not supported between instances of 'float' and 'Float'
[33m[W 2026-01-15 23:52:15,803][0m Trial 0 failed with value None.[0m
Error training TimesNet: '<=' not supported between instances of 'float' and 'Float'
Step: 0 | Action: 0 (TimesNet) | Reward: -1.000000

======================================================================
Step 2/10: Training VanillaTransformer
======================================================================
[32m[I 2026-01-15 23:52:15,843][0m A new study created in memory with name: no-name-4e695836-46ba-4877-b15f-7b1c5d268a2e[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:15,846][0m Trial 0 failed with parameters: {'input_size': 72, 'nhead': 8, 'num_encoder_layers': 3, 'num_decoder_layers': 3, 'batch_size': 64, 'scaler_type': 'minmax'} because of the following error: TypeError("VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:15,846][0m Trial 0 failed with value None.[0m
Error training VanillaTransformer: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
Step: 1 | Action: 1 (VanillaTransformer) | Reward: -1.000000

======================================================================
Step 3/10: Training GRU
======================================================================
[32m[I 2026-01-15 23:52:15,882][0m A new study created in memory with name: no-name-bba1c13c-aef9-41cd-8b7d-559c90371a96[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:15,892][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 256, 'num_layers': 1, 'batch_size': 16} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:15,892][0m Trial 0 failed with value None.[0m
Error training GRU: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 2 | Action: 2 (GRU) | Reward: -1.000000

======================================================================
Step 4/10: Training LSTM
======================================================================
[32m[I 2026-01-15 23:52:15,924][0m A new study created in memory with name: no-name-b0b7cb83-9341-486b-8150-7b6ac4f919d7[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:15,934][0m Trial 0 failed with parameters: {'input_size': 24, 'hidden_size': 64, 'num_layers': 3, 'batch_size': 128} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:15,935][0m Trial 0 failed with value None.[0m
Error training LSTM: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 3 | Action: 3 (LSTM) | Reward: -1.000000

======================================================================
Step 5/10: Training KAN
======================================================================
[32m[I 2026-01-15 23:52:15,965][0m A new study created in memory with name: no-name-1fb2f9d4-bbf6-4f20-8d57-e92ee698d396[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:15,966][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 32, 'grid_size': 15, 'batch_size': 64, 'spline_order': 3, 'scaler_type': 'minmax'} because of the following error: TypeError("KAN.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: KAN.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:15,967][0m Trial 0 failed with value None.[0m
Error training KAN: KAN.__init__() missing 1 required positional argument: 'input_size'
Step: 4 | Action: 4 (KAN) | Reward: -1.000000

======================================================================
Step 6/10: Training TimesNet
======================================================================
[32m[I 2026-01-15 23:52:15,985][0m A new study created in memory with name: no-name-baefd6d0-2579-41fb-8f12-aac50aee1bda[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:625: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate
  warnings.warn(
[33m[W 2026-01-15 23:52:17,051][0m Trial 0 failed with parameters: {'input_size': 48, 'hidden_size': 256, 'dropout': 0.15432059578986707, 'conv_hidden_size': 128, 'top_k': 6, 'num_kernels': 7, 'encoder_layers': 4, 'batch_size': 32, 'windows_batch_size': 128} because of the following error: TypeError("'<=' not supported between instances of 'float' and 'Float'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 602, in _fit
    trainer.fit(model, datamodule=datamodule)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1053, in _run
    self.strategy.setup(self)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 159, in setup
    self.setup_optimizers(trainer)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 180, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 629, in configure_optimizers
    optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/torch/optim/adadelta.py", line 44, in __init__
    if not 0.0 <= lr:
           ^^^^^^^^^
TypeError: '<=' not supported between instances of 'float' and 'Float'
[33m[W 2026-01-15 23:52:17,052][0m Trial 0 failed with value None.[0m
Error training TimesNet: '<=' not supported between instances of 'float' and 'Float'
Step: 5 | Action: 0 (TimesNet) | Reward: -1.000000

======================================================================
Step 7/10: Training VanillaTransformer
======================================================================
[32m[I 2026-01-15 23:52:17,078][0m A new study created in memory with name: no-name-fbb80b7e-e092-4d1f-9663-43730e7d0527[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:17,079][0m Trial 0 failed with parameters: {'input_size': 48, 'nhead': 2, 'num_encoder_layers': 1, 'num_decoder_layers': 2, 'batch_size': 64, 'scaler_type': 'standard'} because of the following error: TypeError("VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:17,080][0m Trial 0 failed with value None.[0m
Error training VanillaTransformer: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
Step: 6 | Action: 1 (VanillaTransformer) | Reward: -1.000000

======================================================================
Step 8/10: Training GRU
======================================================================
[32m[I 2026-01-15 23:52:17,102][0m A new study created in memory with name: no-name-9f634ff2-d5a0-4723-9359-88152276a265[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:17,105][0m Trial 0 failed with parameters: {'input_size': 48, 'hidden_size': 64, 'num_layers': 1, 'batch_size': 16} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:17,106][0m Trial 0 failed with value None.[0m
Error training GRU: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 7 | Action: 2 (GRU) | Reward: -1.000000

======================================================================
Step 9/10: Training LSTM
======================================================================
[32m[I 2026-01-15 23:52:17,125][0m A new study created in memory with name: no-name-8e48da0c-dc7a-46fa-9031-0608fc30380e[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:17,129][0m Trial 0 failed with parameters: {'input_size': 24, 'hidden_size': 128, 'num_layers': 1, 'batch_size': 16} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:17,129][0m Trial 0 failed with value None.[0m
Error training LSTM: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 8 | Action: 3 (LSTM) | Reward: -1.000000

======================================================================
Step 10/10: Training KAN
======================================================================
[32m[I 2026-01-15 23:52:17,147][0m A new study created in memory with name: no-name-2bf24a9a-851b-4d3c-9cf2-d03c74fbd518[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:17,149][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 256, 'grid_size': 5, 'batch_size': 128, 'spline_order': 4, 'scaler_type': 'standard'} because of the following error: TypeError("KAN.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: KAN.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:17,149][0m Trial 0 failed with value None.[0m
Error training KAN: KAN.__init__() missing 1 required positional argument: 'input_size'
Step: 9 | Action: 4 (KAN) | Reward: -1.000000
HenÃ¼z sonuÃ§ yok!

=== Episode 2/5 ===

======================================================================
Environment Reset
======================================================================


======================================================================
Step 1/10: Training TimesNet
======================================================================
[32m[I 2026-01-15 23:52:17,167][0m A new study created in memory with name: no-name-1af52ec9-9be5-4c84-9020-dd74d18f9cf7[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:625: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate
  warnings.warn(
[33m[W 2026-01-15 23:52:17,206][0m Trial 0 failed with parameters: {'input_size': 48, 'hidden_size': 256, 'dropout': 0.11361213401951359, 'conv_hidden_size': 16, 'top_k': 3, 'num_kernels': 7, 'encoder_layers': 1, 'batch_size': 32, 'windows_batch_size': 128} because of the following error: TypeError("'<=' not supported between instances of 'float' and 'Float'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 602, in _fit
    trainer.fit(model, datamodule=datamodule)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1053, in _run
    self.strategy.setup(self)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 159, in setup
    self.setup_optimizers(trainer)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 180, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 629, in configure_optimizers
    optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/torch/optim/adadelta.py", line 44, in __init__
    if not 0.0 <= lr:
           ^^^^^^^^^
TypeError: '<=' not supported between instances of 'float' and 'Float'
[33m[W 2026-01-15 23:52:17,206][0m Trial 0 failed with value None.[0m
Error training TimesNet: '<=' not supported between instances of 'float' and 'Float'
Step: 10 | Action: 0 (TimesNet) | Reward: -1.000000

======================================================================
Step 2/10: Training VanillaTransformer
======================================================================
[32m[I 2026-01-15 23:52:17,239][0m A new study created in memory with name: no-name-f04591fc-dc4d-437a-bb05-a0523b8cbd87[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:17,241][0m Trial 0 failed with parameters: {'input_size': 48, 'nhead': 4, 'num_encoder_layers': 1, 'num_decoder_layers': 4, 'batch_size': 16, 'scaler_type': 'standard'} because of the following error: TypeError("VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:17,241][0m Trial 0 failed with value None.[0m
Error training VanillaTransformer: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
Step: 11 | Action: 1 (VanillaTransformer) | Reward: -1.000000

======================================================================
Step 3/10: Training GRU
======================================================================
[32m[I 2026-01-15 23:52:17,258][0m A new study created in memory with name: no-name-de7d7bb3-51a2-494e-aa9a-6a97883b7cdf[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:17,262][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 256, 'num_layers': 2, 'batch_size': 16} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:17,262][0m Trial 0 failed with value None.[0m
Error training GRU: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 12 | Action: 2 (GRU) | Reward: -1.000000

======================================================================
Step 4/10: Training LSTM
======================================================================
[32m[I 2026-01-15 23:52:17,280][0m A new study created in memory with name: no-name-b2f91713-09f8-47da-9aaa-1026614d5c41[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:17,284][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 128, 'num_layers': 1, 'batch_size': 32} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:17,284][0m Trial 0 failed with value None.[0m
Error training LSTM: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 13 | Action: 3 (LSTM) | Reward: -1.000000

======================================================================
Step 5/10: Training KAN
======================================================================
[32m[I 2026-01-15 23:52:17,301][0m A new study created in memory with name: no-name-cee2f757-98a6-4762-9e8a-f9fc6a00deab[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:17,302][0m Trial 0 failed with parameters: {'input_size': 24, 'hidden_size': 256, 'grid_size': 20, 'batch_size': 16, 'spline_order': 3, 'scaler_type': 'minmax'} because of the following error: TypeError("KAN.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: KAN.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:17,302][0m Trial 0 failed with value None.[0m
Error training KAN: KAN.__init__() missing 1 required positional argument: 'input_size'
Step: 14 | Action: 4 (KAN) | Reward: -1.000000

======================================================================
Step 6/10: Training TimesNet
======================================================================
[32m[I 2026-01-15 23:52:17,321][0m A new study created in memory with name: no-name-6d2baa44-a66c-45ad-9360-a4074b06b662[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:625: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate
  warnings.warn(
[33m[W 2026-01-15 23:52:17,395][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 128, 'dropout': 0.018896769973138883, 'conv_hidden_size': 16, 'top_k': 4, 'num_kernels': 8, 'encoder_layers': 3, 'batch_size': 32, 'windows_batch_size': 256} because of the following error: TypeError("'<=' not supported between instances of 'float' and 'Float'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 602, in _fit
    trainer.fit(model, datamodule=datamodule)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1053, in _run
    self.strategy.setup(self)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 159, in setup
    self.setup_optimizers(trainer)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 180, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 629, in configure_optimizers
    optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/torch/optim/adadelta.py", line 44, in __init__
    if not 0.0 <= lr:
           ^^^^^^^^^
TypeError: '<=' not supported between instances of 'float' and 'Float'
[33m[W 2026-01-15 23:52:17,395][0m Trial 0 failed with value None.[0m
Error training TimesNet: '<=' not supported between instances of 'float' and 'Float'
Step: 15 | Action: 0 (TimesNet) | Reward: -1.000000

======================================================================
Step 7/10: Training VanillaTransformer
======================================================================
[32m[I 2026-01-15 23:52:17,415][0m A new study created in memory with name: no-name-5c6cc8af-27bf-477f-8a4d-2599b7d186bd[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:17,417][0m Trial 0 failed with parameters: {'input_size': 48, 'nhead': 4, 'num_encoder_layers': 1, 'num_decoder_layers': 3, 'batch_size': 64, 'scaler_type': 'minmax'} because of the following error: TypeError("VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:17,417][0m Trial 0 failed with value None.[0m
Error training VanillaTransformer: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
Step: 16 | Action: 1 (VanillaTransformer) | Reward: -1.000000

======================================================================
Step 8/10: Training GRU
======================================================================
[32m[I 2026-01-15 23:52:17,436][0m A new study created in memory with name: no-name-27b43f1c-36f4-4a14-b44c-200399be4596[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:17,439][0m Trial 0 failed with parameters: {'input_size': 24, 'hidden_size': 128, 'num_layers': 2, 'batch_size': 16} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:17,439][0m Trial 0 failed with value None.[0m
Error training GRU: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 17 | Action: 2 (GRU) | Reward: -1.000000

======================================================================
Step 9/10: Training LSTM
======================================================================
[32m[I 2026-01-15 23:52:17,456][0m A new study created in memory with name: no-name-1ff932de-02c6-4f56-8e16-d469704631f6[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:17,458][0m Trial 0 failed with parameters: {'input_size': 48, 'hidden_size': 64, 'num_layers': 2, 'batch_size': 64} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:17,458][0m Trial 0 failed with value None.[0m
Error training LSTM: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 18 | Action: 3 (LSTM) | Reward: -1.000000

======================================================================
Step 10/10: Training KAN
======================================================================
[32m[I 2026-01-15 23:52:17,476][0m A new study created in memory with name: no-name-8ac08986-75ac-4cf3-bfed-2c0a5e7a1227[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:17,477][0m Trial 0 failed with parameters: {'input_size': 48, 'hidden_size': 128, 'grid_size': 15, 'batch_size': 16, 'spline_order': 2, 'scaler_type': 'minmax'} because of the following error: TypeError("KAN.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: KAN.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:17,477][0m Trial 0 failed with value None.[0m
Error training KAN: KAN.__init__() missing 1 required positional argument: 'input_size'
Step: 19 | Action: 4 (KAN) | Reward: -1.000000
HenÃ¼z sonuÃ§ yok!

=== Episode 3/5 ===

======================================================================
Environment Reset
======================================================================


======================================================================
Step 1/10: Training TimesNet
======================================================================
[32m[I 2026-01-15 23:52:17,494][0m A new study created in memory with name: no-name-8a282f4f-0dfb-4651-85ee-1f6c6e6f8e8c[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:625: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate
  warnings.warn(
[33m[W 2026-01-15 23:52:17,545][0m Trial 0 failed with parameters: {'input_size': 24, 'hidden_size': 64, 'dropout': 0.27874048800086554, 'conv_hidden_size': 32, 'top_k': 7, 'num_kernels': 7, 'encoder_layers': 3, 'batch_size': 32, 'windows_batch_size': 1024} because of the following error: TypeError("'<=' not supported between instances of 'float' and 'Float'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 602, in _fit
    trainer.fit(model, datamodule=datamodule)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1053, in _run
    self.strategy.setup(self)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 159, in setup
    self.setup_optimizers(trainer)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 180, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 629, in configure_optimizers
    optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/torch/optim/adadelta.py", line 44, in __init__
    if not 0.0 <= lr:
           ^^^^^^^^^
TypeError: '<=' not supported between instances of 'float' and 'Float'
[33m[W 2026-01-15 23:52:17,545][0m Trial 0 failed with value None.[0m
Error training TimesNet: '<=' not supported between instances of 'float' and 'Float'
Step: 20 | Action: 0 (TimesNet) | Reward: -1.000000

======================================================================
Step 2/10: Training VanillaTransformer
======================================================================
[32m[I 2026-01-15 23:52:17,562][0m A new study created in memory with name: no-name-77ce1acd-cb9b-41bc-828c-089e25306269[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:17,564][0m Trial 0 failed with parameters: {'input_size': 24, 'nhead': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'batch_size': 16, 'scaler_type': 'minmax'} because of the following error: TypeError("VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:17,564][0m Trial 0 failed with value None.[0m
Error training VanillaTransformer: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
Step: 21 | Action: 1 (VanillaTransformer) | Reward: -1.000000

======================================================================
Step 3/10: Training GRU
======================================================================
[32m[I 2026-01-15 23:52:17,581][0m A new study created in memory with name: no-name-21bda79c-ea96-420b-baee-a2a24f8938c2[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:17,585][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 256, 'num_layers': 3, 'batch_size': 64} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:17,586][0m Trial 0 failed with value None.[0m
Error training GRU: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 22 | Action: 2 (GRU) | Reward: -1.000000

======================================================================
Step 4/10: Training LSTM
======================================================================
[32m[I 2026-01-15 23:52:17,602][0m A new study created in memory with name: no-name-6aec122e-375e-4a45-bb25-70d7c9b54ff4[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:17,607][0m Trial 0 failed with parameters: {'input_size': 48, 'hidden_size': 256, 'num_layers': 1, 'batch_size': 64} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:17,607][0m Trial 0 failed with value None.[0m
Error training LSTM: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 23 | Action: 3 (LSTM) | Reward: -1.000000

======================================================================
Step 5/10: Training KAN
======================================================================
[32m[I 2026-01-15 23:52:17,623][0m A new study created in memory with name: no-name-01f6ef26-b24d-45c8-bcc1-dcae50fca610[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:17,625][0m Trial 0 failed with parameters: {'input_size': 48, 'hidden_size': 32, 'grid_size': 5, 'batch_size': 128, 'spline_order': 4, 'scaler_type': 'robust'} because of the following error: TypeError("KAN.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: KAN.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:17,625][0m Trial 0 failed with value None.[0m
Error training KAN: KAN.__init__() missing 1 required positional argument: 'input_size'
Step: 24 | Action: 4 (KAN) | Reward: -1.000000

======================================================================
Step 6/10: Training TimesNet
======================================================================
[32m[I 2026-01-15 23:52:17,642][0m A new study created in memory with name: no-name-35d37c7a-57d3-4ca8-84e6-9b96813066bd[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:625: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate
  warnings.warn(
[33m[W 2026-01-15 23:52:17,709][0m Trial 0 failed with parameters: {'input_size': 96, 'hidden_size': 32, 'dropout': 0.0551216343778157, 'conv_hidden_size': 64, 'top_k': 5, 'num_kernels': 8, 'encoder_layers': 3, 'batch_size': 32, 'windows_batch_size': 1024} because of the following error: TypeError("'<=' not supported between instances of 'float' and 'Float'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 602, in _fit
    trainer.fit(model, datamodule=datamodule)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1053, in _run
    self.strategy.setup(self)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 159, in setup
    self.setup_optimizers(trainer)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 180, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 629, in configure_optimizers
    optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/torch/optim/adadelta.py", line 44, in __init__
    if not 0.0 <= lr:
           ^^^^^^^^^
TypeError: '<=' not supported between instances of 'float' and 'Float'
[33m[W 2026-01-15 23:52:17,709][0m Trial 0 failed with value None.[0m
Error training TimesNet: '<=' not supported between instances of 'float' and 'Float'
Step: 25 | Action: 0 (TimesNet) | Reward: -1.000000

======================================================================
Step 7/10: Training VanillaTransformer
======================================================================
[32m[I 2026-01-15 23:52:17,729][0m A new study created in memory with name: no-name-6a445677-2d2a-4a25-bda7-7fbfd059843a[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:17,730][0m Trial 0 failed with parameters: {'input_size': 96, 'nhead': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 1, 'batch_size': 16, 'scaler_type': 'standard'} because of the following error: TypeError("VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:17,730][0m Trial 0 failed with value None.[0m
Error training VanillaTransformer: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
Step: 26 | Action: 1 (VanillaTransformer) | Reward: -1.000000

======================================================================
Step 8/10: Training GRU
======================================================================
[32m[I 2026-01-15 23:52:17,748][0m A new study created in memory with name: no-name-6f2f41ed-c7ef-4dd1-a6d8-8baa7ae98ea3[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:17,750][0m Trial 0 failed with parameters: {'input_size': 24, 'hidden_size': 32, 'num_layers': 2, 'batch_size': 16} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:17,751][0m Trial 0 failed with value None.[0m
Error training GRU: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 27 | Action: 2 (GRU) | Reward: -1.000000

======================================================================
Step 9/10: Training LSTM
======================================================================
[32m[I 2026-01-15 23:52:17,767][0m A new study created in memory with name: no-name-8d9f57c5-7385-4e22-bc87-9674ab8ba694[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:17,770][0m Trial 0 failed with parameters: {'input_size': 24, 'hidden_size': 32, 'num_layers': 2, 'batch_size': 64} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:17,771][0m Trial 0 failed with value None.[0m
Error training LSTM: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 28 | Action: 3 (LSTM) | Reward: -1.000000

======================================================================
Step 10/10: Training KAN
======================================================================
[32m[I 2026-01-15 23:52:17,788][0m A new study created in memory with name: no-name-fb71191a-be12-4949-bfdd-5043e4c8d9f9[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:17,790][0m Trial 0 failed with parameters: {'input_size': 24, 'hidden_size': 32, 'grid_size': 20, 'batch_size': 32, 'spline_order': 3, 'scaler_type': 'standard'} because of the following error: TypeError("KAN.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: KAN.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:17,790][0m Trial 0 failed with value None.[0m
Error training KAN: KAN.__init__() missing 1 required positional argument: 'input_size'
Step: 29 | Action: 4 (KAN) | Reward: -1.000000
HenÃ¼z sonuÃ§ yok!

=== Episode 4/5 ===

======================================================================
Environment Reset
======================================================================


======================================================================
Step 1/10: Training TimesNet
======================================================================
[32m[I 2026-01-15 23:52:17,807][0m A new study created in memory with name: no-name-e5cc8d7e-79de-4c6a-882b-4389073361ac[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:625: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate
  warnings.warn(
[33m[W 2026-01-15 23:52:18,576][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 256, 'dropout': 0.19948257036906342, 'conv_hidden_size': 64, 'top_k': 7, 'num_kernels': 8, 'encoder_layers': 4, 'batch_size': 128, 'windows_batch_size': 256} because of the following error: TypeError("'<=' not supported between instances of 'float' and 'Float'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 602, in _fit
    trainer.fit(model, datamodule=datamodule)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1053, in _run
    self.strategy.setup(self)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 159, in setup
    self.setup_optimizers(trainer)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 180, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 629, in configure_optimizers
    optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/torch/optim/adadelta.py", line 44, in __init__
    if not 0.0 <= lr:
           ^^^^^^^^^
TypeError: '<=' not supported between instances of 'float' and 'Float'
[33m[W 2026-01-15 23:52:18,577][0m Trial 0 failed with value None.[0m
Error training TimesNet: '<=' not supported between instances of 'float' and 'Float'
Step: 30 | Action: 0 (TimesNet) | Reward: -1.000000

======================================================================
Step 2/10: Training VanillaTransformer
======================================================================
[32m[I 2026-01-15 23:52:18,599][0m A new study created in memory with name: no-name-75793b5a-d610-4902-8629-6532e6d2e0b3[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:18,601][0m Trial 0 failed with parameters: {'input_size': 72, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 3, 'batch_size': 128, 'scaler_type': 'robust'} because of the following error: TypeError("VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:18,601][0m Trial 0 failed with value None.[0m
Error training VanillaTransformer: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
Step: 31 | Action: 1 (VanillaTransformer) | Reward: -1.000000

======================================================================
Step 3/10: Training GRU
======================================================================
[32m[I 2026-01-15 23:52:18,619][0m A new study created in memory with name: no-name-91241d4f-d9c5-4f65-b0fb-1f4cda89400d[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:18,622][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 64, 'num_layers': 3, 'batch_size': 16} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:18,622][0m Trial 0 failed with value None.[0m
Error training GRU: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 32 | Action: 2 (GRU) | Reward: -1.000000

======================================================================
Step 4/10: Training LSTM
======================================================================
[32m[I 2026-01-15 23:52:18,639][0m A new study created in memory with name: no-name-ead4a574-7462-4ea3-ac16-702edd0ac168[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:18,643][0m Trial 0 failed with parameters: {'input_size': 24, 'hidden_size': 128, 'num_layers': 2, 'batch_size': 16} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:18,643][0m Trial 0 failed with value None.[0m
Error training LSTM: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 33 | Action: 3 (LSTM) | Reward: -1.000000

======================================================================
Step 5/10: Training KAN
======================================================================
[32m[I 2026-01-15 23:52:18,660][0m A new study created in memory with name: no-name-c93cdc88-e29d-49be-acef-5f2012917a10[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:18,661][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 32, 'grid_size': 5, 'batch_size': 32, 'spline_order': 3, 'scaler_type': 'robust'} because of the following error: TypeError("KAN.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: KAN.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:18,662][0m Trial 0 failed with value None.[0m
Error training KAN: KAN.__init__() missing 1 required positional argument: 'input_size'
Step: 34 | Action: 4 (KAN) | Reward: -1.000000

======================================================================
Step 6/10: Training TimesNet
======================================================================
[32m[I 2026-01-15 23:52:18,679][0m A new study created in memory with name: no-name-172e1563-0002-40ac-9104-4aa8087ea78d[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:625: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate
  warnings.warn(
[33m[W 2026-01-15 23:52:18,728][0m Trial 0 failed with parameters: {'input_size': 48, 'hidden_size': 128, 'dropout': 0.4227766431386863, 'conv_hidden_size': 16, 'top_k': 4, 'num_kernels': 7, 'encoder_layers': 3, 'batch_size': 32, 'windows_batch_size': 1024} because of the following error: TypeError("'<=' not supported between instances of 'float' and 'Float'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 602, in _fit
    trainer.fit(model, datamodule=datamodule)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1053, in _run
    self.strategy.setup(self)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 159, in setup
    self.setup_optimizers(trainer)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 180, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 629, in configure_optimizers
    optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/torch/optim/adadelta.py", line 44, in __init__
    if not 0.0 <= lr:
           ^^^^^^^^^
TypeError: '<=' not supported between instances of 'float' and 'Float'
[33m[W 2026-01-15 23:52:18,728][0m Trial 0 failed with value None.[0m
Error training TimesNet: '<=' not supported between instances of 'float' and 'Float'
Step: 35 | Action: 0 (TimesNet) | Reward: -1.000000

======================================================================
Step 7/10: Training VanillaTransformer
======================================================================
[32m[I 2026-01-15 23:52:18,753][0m A new study created in memory with name: no-name-fda581f4-3a43-4365-be3e-781eca5f4bdc[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:18,756][0m Trial 0 failed with parameters: {'input_size': 48, 'nhead': 2, 'num_encoder_layers': 4, 'num_decoder_layers': 4, 'batch_size': 16, 'scaler_type': 'robust'} because of the following error: TypeError("VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:18,756][0m Trial 0 failed with value None.[0m
Error training VanillaTransformer: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
Step: 36 | Action: 1 (VanillaTransformer) | Reward: -1.000000

======================================================================
Step 8/10: Training GRU
======================================================================
[32m[I 2026-01-15 23:52:18,773][0m A new study created in memory with name: no-name-9a096bf9-87d7-47dd-bec2-67f68ef943c5[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:18,777][0m Trial 0 failed with parameters: {'input_size': 96, 'hidden_size': 256, 'num_layers': 1, 'batch_size': 16} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:18,777][0m Trial 0 failed with value None.[0m
Error training GRU: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 37 | Action: 2 (GRU) | Reward: -1.000000

======================================================================
Step 9/10: Training LSTM
======================================================================
[32m[I 2026-01-15 23:52:18,794][0m A new study created in memory with name: no-name-26ef8ba9-4390-4ca1-94e2-082a21bc142c[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:18,800][0m Trial 0 failed with parameters: {'input_size': 24, 'hidden_size': 256, 'num_layers': 3, 'batch_size': 32} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:18,800][0m Trial 0 failed with value None.[0m
Error training LSTM: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 38 | Action: 3 (LSTM) | Reward: -1.000000

======================================================================
Step 10/10: Training KAN
======================================================================
[32m[I 2026-01-15 23:52:18,817][0m A new study created in memory with name: no-name-05b238f6-d8bb-4b72-bb19-167d75b79172[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:18,818][0m Trial 0 failed with parameters: {'input_size': 48, 'hidden_size': 128, 'grid_size': 15, 'batch_size': 32, 'spline_order': 4, 'scaler_type': 'robust'} because of the following error: TypeError("KAN.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: KAN.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:18,818][0m Trial 0 failed with value None.[0m
Error training KAN: KAN.__init__() missing 1 required positional argument: 'input_size'
Step: 39 | Action: 4 (KAN) | Reward: -1.000000
HenÃ¼z sonuÃ§ yok!

=== Episode 5/5 ===

======================================================================
Environment Reset
======================================================================


======================================================================
Step 1/10: Training TimesNet
======================================================================
[32m[I 2026-01-15 23:52:18,836][0m A new study created in memory with name: no-name-835f03a8-0f24-4bae-b625-106cee8c60df[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:625: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate
  warnings.warn(
[33m[W 2026-01-15 23:52:19,361][0m Trial 0 failed with parameters: {'input_size': 96, 'hidden_size': 128, 'dropout': 0.10046853830242813, 'conv_hidden_size': 128, 'top_k': 5, 'num_kernels': 8, 'encoder_layers': 3, 'batch_size': 16, 'windows_batch_size': 128} because of the following error: TypeError("'<=' not supported between instances of 'float' and 'Float'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 602, in _fit
    trainer.fit(model, datamodule=datamodule)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1053, in _run
    self.strategy.setup(self)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 159, in setup
    self.setup_optimizers(trainer)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 180, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 629, in configure_optimizers
    optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/torch/optim/adadelta.py", line 44, in __init__
    if not 0.0 <= lr:
           ^^^^^^^^^
TypeError: '<=' not supported between instances of 'float' and 'Float'
[33m[W 2026-01-15 23:52:19,361][0m Trial 0 failed with value None.[0m
Error training TimesNet: '<=' not supported between instances of 'float' and 'Float'
Step: 40 | Action: 0 (TimesNet) | Reward: -1.000000

======================================================================
Step 2/10: Training VanillaTransformer
======================================================================
[32m[I 2026-01-15 23:52:19,381][0m A new study created in memory with name: no-name-c851ba90-018e-4121-be01-3f5904eaab7e[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:19,382][0m Trial 0 failed with parameters: {'input_size': 72, 'nhead': 2, 'num_encoder_layers': 4, 'num_decoder_layers': 4, 'batch_size': 16, 'scaler_type': 'robust'} because of the following error: TypeError("VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:19,382][0m Trial 0 failed with value None.[0m
Error training VanillaTransformer: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
Step: 41 | Action: 1 (VanillaTransformer) | Reward: -1.000000

======================================================================
Step 3/10: Training GRU
======================================================================
[32m[I 2026-01-15 23:52:19,399][0m A new study created in memory with name: no-name-53ed079f-b21d-42ef-b9e6-b272f4d89d5b[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:19,402][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 32, 'num_layers': 3, 'batch_size': 128} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:19,402][0m Trial 0 failed with value None.[0m
Error training GRU: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 42 | Action: 2 (GRU) | Reward: -1.000000

======================================================================
Step 4/10: Training LSTM
======================================================================
[32m[I 2026-01-15 23:52:19,419][0m A new study created in memory with name: no-name-5575b02a-d25c-4a87-ad7b-69e05d579710[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:19,421][0m Trial 0 failed with parameters: {'input_size': 96, 'hidden_size': 32, 'num_layers': 3, 'batch_size': 128} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:19,421][0m Trial 0 failed with value None.[0m
Error training LSTM: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 43 | Action: 3 (LSTM) | Reward: -1.000000

======================================================================
Step 5/10: Training KAN
======================================================================
[32m[I 2026-01-15 23:52:19,437][0m A new study created in memory with name: no-name-30fdc8fc-29f3-4154-af2e-43fbbd21219e[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:19,439][0m Trial 0 failed with parameters: {'input_size': 96, 'hidden_size': 64, 'grid_size': 20, 'batch_size': 128, 'spline_order': 2, 'scaler_type': 'minmax'} because of the following error: TypeError("KAN.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: KAN.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:19,439][0m Trial 0 failed with value None.[0m
Error training KAN: KAN.__init__() missing 1 required positional argument: 'input_size'
Step: 44 | Action: 4 (KAN) | Reward: -1.000000

======================================================================
Step 6/10: Training TimesNet
======================================================================
[32m[I 2026-01-15 23:52:19,455][0m A new study created in memory with name: no-name-95cadb1d-8cfb-4c3f-aa50-5106438a3593[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:625: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate
  warnings.warn(
[33m[W 2026-01-15 23:52:19,477][0m Trial 0 failed with parameters: {'input_size': 48, 'hidden_size': 64, 'dropout': 0.09198493113569411, 'conv_hidden_size': 32, 'top_k': 7, 'num_kernels': 7, 'encoder_layers': 1, 'batch_size': 32, 'windows_batch_size': 512} because of the following error: TypeError("'<=' not supported between instances of 'float' and 'Float'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 602, in _fit
    trainer.fit(model, datamodule=datamodule)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1053, in _run
    self.strategy.setup(self)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 159, in setup
    self.setup_optimizers(trainer)
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 180, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 629, in configure_optimizers
    optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/torch/optim/adadelta.py", line 44, in __init__
    if not 0.0 <= lr:
           ^^^^^^^^^
TypeError: '<=' not supported between instances of 'float' and 'Float'
[33m[W 2026-01-15 23:52:19,477][0m Trial 0 failed with value None.[0m
Error training TimesNet: '<=' not supported between instances of 'float' and 'Float'
Step: 45 | Action: 0 (TimesNet) | Reward: -1.000000

======================================================================
Step 7/10: Training VanillaTransformer
======================================================================
[32m[I 2026-01-15 23:52:19,494][0m A new study created in memory with name: no-name-be8c59ad-0195-4998-8387-fb2f4f080354[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:19,495][0m Trial 0 failed with parameters: {'input_size': 24, 'nhead': 4, 'num_encoder_layers': 1, 'num_decoder_layers': 3, 'batch_size': 64, 'scaler_type': 'robust'} because of the following error: TypeError("VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:19,495][0m Trial 0 failed with value None.[0m
Error training VanillaTransformer: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'
Step: 46 | Action: 1 (VanillaTransformer) | Reward: -1.000000

======================================================================
Step 8/10: Training GRU
======================================================================
[32m[I 2026-01-15 23:52:19,512][0m A new study created in memory with name: no-name-f27f30c9-3cf2-4892-ba0a-b2d030da4e33[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:19,515][0m Trial 0 failed with parameters: {'input_size': 72, 'hidden_size': 128, 'num_layers': 2, 'batch_size': 64} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:19,515][0m Trial 0 failed with value None.[0m
Error training GRU: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 47 | Action: 2 (GRU) | Reward: -1.000000

======================================================================
Step 9/10: Training LSTM
======================================================================
[32m[I 2026-01-15 23:52:19,532][0m A new study created in memory with name: no-name-ef1450b9-3bc8-4cb2-9953-bae92fe4c373[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py:151: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 60
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
Seed set to 26
[33m[W 2026-01-15 23:52:19,535][0m Trial 0 failed with parameters: {'input_size': 48, 'hidden_size': 32, 'num_layers': 3, 'batch_size': 32} because of the following error: TypeError("Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 347, in _fit_model
    model = model.fit(
            ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 1924, in fit
    return self._fit(
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_model.py", line 601, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
[33m[W 2026-01-15 23:52:19,535][0m Trial 0 failed with value None.[0m
Error training LSTM: Trainer.__init__() got an unexpected keyword argument 'input_size_multiplier'
Step: 48 | Action: 3 (LSTM) | Reward: -1.000000

======================================================================
Step 10/10: Training KAN
======================================================================
[32m[I 2026-01-15 23:52:19,552][0m A new study created in memory with name: no-name-05cfbc6f-7d42-4857-b7dd-e6c4a152f170[0m
[34m[1mwandb[0m: [33mWARNING[0m `wandb.require('service')` is a no-op as it is now the default behavior.
/usr/local/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
[33m[W 2026-01-15 23:52:19,554][0m Trial 0 failed with parameters: {'input_size': 24, 'hidden_size': 128, 'grid_size': 15, 'batch_size': 16, 'spline_order': 3, 'scaler_type': 'robust'} because of the following error: TypeError("KAN.__init__() missing 1 required positional argument: 'input_size'").[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 310, in objective
    model = self._fit_model(
            ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/neuralforecast/common/_base_auto.py", line 346, in _fit_model
    model = cls_model(**config)
            ^^^^^^^^^^^^^^^^^^^
TypeError: KAN.__init__() missing 1 required positional argument: 'input_size'
[33m[W 2026-01-15 23:52:19,554][0m Trial 0 failed with value None.[0m
Error training KAN: KAN.__init__() missing 1 required positional argument: 'input_size'
Step: 49 | Action: 4 (KAN) | Reward: -1.000000
HenÃ¼z sonuÃ§ yok!

======================================================================
STATISTICS
======================================================================
Total Steps: 50
Best Action: 0 (TimesNet)
Action Counts: [10.0, 10.0, 10.0, 10.0, 10.0]
Mean Rewards: ['-1.000000', '-1.000000', '-1.000000', '-1.000000', '-1.000000']
Total Rewards: ['-10.000000', '-10.000000', '-10.000000', '-10.000000', '-10.000000']
======================================================================

â Agent saved: outputs/run_20260115_235212/agent.json

======================================================================
Training ended at 2026-01-15 23:52:19
All outputs saved to: outputs/run_20260115_235212
======================================================================
EÄitim tamamlandÄ±.
